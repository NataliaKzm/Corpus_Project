{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Corpus_Building.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIzElc3Vm5oaVwZy9TiZwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaKzm/Corpus_Project/blob/master/Corpus_Building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6oSsWHjrC7i"
      },
      "source": [
        "# **This tutorial is dedicated for building a corpus:**\n",
        "To automate the process of data collection for building a corpus, we are going to use a web scraping tool built with Python libraries: **Justext** or **BeautifulSoup**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81_ZfhYzr75a"
      },
      "source": [
        "You will see a set of instructions for running two different approaches to build your corpus:\n",
        "\n",
        "\n",
        "*   one works through a Justext library\n",
        "*   another one deals with a BeautifulSoup package\n",
        "\n",
        "**Note** that you need to choose one of them, depending on the source you're working with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvQNtO9BtBij"
      },
      "source": [
        "# **Justext parsing:**\n",
        "\n",
        "This script extract textual information from 10 different URLs (e.g. BBC website) and as the output it provides a .txt file for each link. \n",
        "**BUT**, you  will have to update *the URL* and *the output filename* every time mannualy. If you forget to change the name of the .txt file - a previously extracted text will be rewritten with the new information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHTUvF6trg0d"
      },
      "source": [
        "#First we need to install Justext library\n",
        "#Simply run this cell\n",
        "pip install lxml justext requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm50BlRtrx07"
      },
      "source": [
        "#Then we import the libraries\n",
        "import justext\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGFD-6jBuYu9"
      },
      "source": [
        "#Run this sell to process your first 10 URLs\n",
        "page1 = requests.get('https://www.bbc.co.uk/news/health-26681306').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page2 = requests.get('https://www.bbc.co.uk/news/education-26617395').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page3 = requests.get('https://www.bbc.co.uk/news/magazine-26472906').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page4 = requests.get('https://www.bbc.co.uk/news/magazine-26610276').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page5 = requests.get('https://www.bbc.co.uk/news/magazine-26431858').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page6 = requests.get('https://www.bbc.co.uk/news/science-environment-26172181?').text.encode('utf-8')     #insert a link  for the extracting here\n",
        "page7 = requests.get('https://www.bbc.co.uk/news/science-environment-26325934').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page8 = requests.get('https://www.bbc.co.uk/news/science-environment-26340038').text.encode('utf-8')  #insert a link  for the extracting here\n",
        "page9 = requests.get('https://www.bbc.co.uk/sport/disability-sport/26591726').text.encode('utf-8') #insert a link  for the extracting here\n",
        "page10 = requests.get(' https://www.bbc.co.uk/sport/disability-sport/26549695').text.encode('utf-8')#insert a link  for the extracting here\n",
        "\n",
        "paragraphs = justext.justext(page1, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_1.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "    \n",
        "paragraphs = justext.justext(page2, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    #if paragraph['class'] == 'good': #old\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_2.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page3, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_3.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "\n",
        "paragraphs = justext.justext(page4, justext.get_stoplist('English')) \n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_4.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page5, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_5.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page6, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_6.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page7, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_7.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page8, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BDan_8.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page9, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_9.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)\n",
        "\n",
        "paragraphs = justext.justext(page10, justext.get_stoplist('English'))\n",
        "for paragraph in paragraphs:\n",
        "    if not paragraph.is_boilerplate:\n",
        "#        print(paragraph['text'])        \n",
        "        with open(\"BBC_10.txt\", \"a\") as myfile: #change the name of an output .txt file\n",
        "            myfile.write(paragraph.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2zXtaQ5u8BT"
      },
      "source": [
        "# **BeatufulSoup package:**\n",
        "\n",
        "This library works more efficiently but requires proofreading. It analyses a .txt input file that should be located in your working directory and contains *all your URLs* an then gives one .txt file that contains extracted text from all webpages. \n",
        "\n",
        "**Note**: it can happen that the parser doesn't recognize a text on a webpage. In this case it will skip a link and goes to a next one.\n",
        "\n",
        "Requires: **Linux** or **Ubuntu** system to be installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "m5mbsnAwvo07",
        "outputId": "a0e60814-8b9a-4713-ff9b-7f38c75a8b93"
      },
      "source": [
        "#Install the library:\n",
        "pip install lxml bs4 requests sys webbrowser codecs"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-12aebd9b675b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install lxml bs4 requests sys webbrowser codecs\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A0NyQ6EwNo8"
      },
      "source": [
        "#Import the packages\n",
        "import requests, sys, webbrowser, bs4\n",
        "import codecs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "404pqc66wU0w"
      },
      "source": [
        "def get_content(link):\n",
        "  page = requests.get(link)\n",
        "  soup = bs4.BeautifulSoup(page.content, 'html.parser')\n",
        "  all_p = soup.find_all('p')\n",
        "  content = ''\n",
        "  for p in all_p:\n",
        "    content += p.get_text().strip('\\n')\n",
        "  return content\n",
        "\n",
        "in_path = \"input.txt\"  #the text should be located in the same directory with a script\n",
        "out_path = \"outputData.txt\" #this file will be created\n",
        "\n",
        "with open(in_path, 'r') as fin:\n",
        "  links = fin.read().splitlines()\n",
        "with open(out_path, 'w') as fout:\n",
        "  for i, link in enumerate(links):\n",
        "     fout.write(get_content(link) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}